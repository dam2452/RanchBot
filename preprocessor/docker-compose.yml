version: '3.8'

services:
  preprocessor:
    build:
      context: ..
      dockerfile: preprocessor/Dockerfile
    image: ranczo-preprocessor:latest
    container_name: ranchbot-preprocessing-app

    runtime: nvidia

    env_file:
      - .env

    environment:
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=all
      # Set to 'false' to skip model downloads on startup
      - PULL_EXTRA_MODELS=false
      # Model cache paths (unified under /models)
      - HF_HOME=/models/huggingface
      - TRANSFORMERS_CACHE=/models/huggingface
      - TORCH_HOME=/models/torch
      - WHISPER_CACHE=/models/whisper
      - OLLAMA_MODELS=/models/ollama
      - LD_LIBRARY_PATH=/usr/lib/wsl/drivers/nv_dispi.inf_amd64_4646b47d9477048e:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/lib/python3.11/dist-packages/nvidia/cudnn/lib:/usr/local/lib/python3.11/dist-packages/ctranslate2.libs

    volumes:
      # Input data (read-only)
      - ./videos:/videos:ro
      - ./episodes.json:/app/preprocessor/episodes.json:ro
      - ./.env:/app/.env:ro

      # Output data
      - ./preprocessed:/app/preprocessed

      # =========================================================
      # NVENC SUPPORT (WSL2): Uncomment if you want GPU encoding
      # =========================================================
      # Without this mount, NVENC won't work in WSL2 and CPU codec will be used
      # This is NOT data, it's GPU driver infrastructure (read-only)
      - /usr/lib/wsl/drivers:/usr/lib/wsl/drivers:ro

      # =========================================================
      # PERFORMANCE: RAMDISK for temporary files (64GB RAM)
      # =========================================================
      # Uncomment for 20-30% faster audio normalization
      # Requires: 16GB free RAM, tmpfs mount on host
      # Setup: sudo mount -t tmpfs -o size=16G tmpfs /mnt/ramdisk
      # - /mnt/ramdisk:/mnt/ramdisk

      # =========================================================
      # MODEL CACHE VOLUME - persist across rebuilds!
      # =========================================================
      # All ML models (HuggingFace, Whisper, TransNet, Torch, Ollama)
      - ml_models:/models

    ports:
      - "11434:11434"

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu, video, compute, utility]

    stdin_open: true
    tty: true

    command: bash

# =============================================================================
# NAMED VOLUMES - Persist on the host filesystem!
# =============================================================================
# Location: /var/lib/docker/volumes/ranczo-ml-models
#
# To inspect: docker volume inspect ranczo-ml-models
# To backup:  docker run --rm -v ml_models:/data -v $(pwd):/backup alpine tar czf /backup/models.tar.gz /data
# To clear:   docker volume rm ranczo-ml-models
# =============================================================================
volumes:
  ml_models:
    name: ranchbot-ai-models
    driver: local
